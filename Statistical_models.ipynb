{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-fR1qHaV5Yi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "!ls \"/content/drive/My Drive/length of stay\"\n",
        "!cd \"/content/drive/My Drive/length of stay\"\n",
        "print(os.getcwd())\n",
        "os.chdir(\"/content/drive/My Drive/length of stay\")\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjkfd11LV9qk"
      },
      "source": [
        "## installing required libraries\n",
        "# !pip install xgboost\n",
        "# !pip install lightgbm\n",
        "# bayesian-optimization\n",
        "# !pip install scikit-optimize\n",
        "\n",
        "## Defining required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import losses\n",
        "from keras.layers import Dropout\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import keras.backend as K\n",
        "from data_preprocessing import gendata",
        "# Reading tabular data\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "\n",

        "train_X, test_X, train_Y, test_Y = gendata(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUWuntUnWFZt"
      },
      "source": [
        "############ RESPONSE TO CHALLENGE 0 - Missing values - designing estimator for grid search to remove unwanted cols ###############\n",
        "\n",
        "# define a new sklearn-supported class to be useable in pipeline\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "#\n",
        "class remove_m_highlymissed_values(BaseEstimator, ClassifierMixin):\n",
        "  '''\n",
        "  An estimator to remove highly noisy missing valued columns that even imputation will not work on them\n",
        "  '''\n",
        "\n",
        " ## Designing constructor\n",
        "  def __init__(self, m=0):\n",
        "     self.m = m\n",
        "\n",
        " ## designing fit function. Here we only need to find and sort the highly missed ...\n",
        " ## columns (cols with highest num of missing vals). THen remove it from output dataframe \n",
        "  def fit(self, X, y):\n",
        "     self.cols_indices_sorted = np.argsort(X.isnull().sum().values)[::-1]\n",
        "     self.X_ = X.iloc[:,self.cols_indices_sorted[self.m+1:]]\n",
        "     self.y_ = y\n",
        "     return self\n",
        "\n",
        "  def \n",
        "  (self, X):\n",
        "     # Input validation\n",
        "    #  X = check_array(X)\n",
        "     self.cols_indices_sorted = np.argsort(X.isnull().sum().values)[::-1]\n",
        "     return X.iloc[:,self.cols_indices_sorted[self.m+1:]]\n",
        "\n",
        "  def transform(self, X):\n",
        "     # Input validation\n",
        "    #  X = check_array(X)\n",
        "     self.cols_indices_sorted = np.argsort(X.isnull().sum().values)[::-1]\n",
        "     return X.iloc[:,self.cols_indices_sorted[self.m+1:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYkGsLodWG0U",
        "outputId": "ce19885d-93b6-41d0-c2d8-7ca7e4884408"
      },
      "source": [
        "\n",
        "############## RESPONSE TO CHALLENGE 3 -  IMBALANCE CLASSES - Result: Using down sampler to reduce number of samples ###############\n",
        "\n",
        "np.histogram(train_Y)\n",
        "# frequency of unmatched is 5 time the freq of matched in the labels. ...\n",
        "# So unbalanced. To tackle this problem, I use SMOTT and ALYASIN, and undersampler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([24709, 24566,  9731, 14123,  3754,  2554,   457,    61,    37,\n",
              "            8]),\n",
              " array([ 1. ,  2.6,  4.2,  5.8,  7.4,  9. , 10.6, 12.2, 13.8, 15.4, 17. ]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC6FV0yLWIW4"
      },
      "source": [
        "############## Deal with Preprocessing of numeric and categorical features apart ##############\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# random generators\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        "\n",
        "# importing required sklearn tools for pipelining, sampling and missing value replacement\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "# from imblearn.pipeline import Pipeline # as we want downsampler to work, we use imblearn (extend of sklearn.pipeline)\n",
        "from imblearn.pipeline import Pipeline # as we want downsampler to work, we use imblearn (extend of sklearn.pipeline)\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# importing evaluators\n",
        "from sklearn.metrics import f1_score # geometric mean (of prec and rec) always detects model weekness if there is one in either prec or recall, as it is less than least of the two.\n",
        "from sklearn.metrics import roc_auc_score # area under the curve is a powerful metric for assuring that the model has high true-positives while manifesting fp values in bootstraps or folds\n",
        "\n",
        "# importing print and plot tools\n",
        "from pprint import pprint\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# importing required classifiers:\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBClassifier # xgboost should obey the syntax of sklearn to be used in sklearn pipeline class\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# make the random number predictable, i.e. the same in each run. Set constant initial number (seed)\n",
        "np.random.seed(0) ##TODO : REMOVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXBMHnk2WKOp"
      },
      "source": [
        "# xgboost classification pipeline\n",
        "full_pipline_xgb = Pipeline(\n",
        "    steps=[\n",
        "        #('sampler', RandomUnderSampler(random_state=42)),\n",
        "        ('my_classifier', XGBClassifier(seed=1)) ##TODO: SEED\n",
        "    ]\n",
        ") \n",
        "\n",
        "# Random forest classification pipeline\n",
        "full_pipline_rf = Pipeline(\n",
        "    steps=[\n",
        "        #('sampler', RandomUnderSampler(random_state=42)),\n",
        "        ('my_classifier', RandomForestRegressor(\n",
        "            n_estimators=100))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# lightgbm classification pipeline \n",
        "full_pipline_lgbm = Pipeline(\n",
        "    steps=[\n",
        "        #('sampler', RandomUnderSampler(random_state=42)),\n",
        "        ('my_classifier', LGBMClassifier(n_jobs=-1, n_estimators=7000, max_bin= [128],num_leaves= [8],reg_alpha= [1.2],reg_lambda= [1.2],min_data_in_leaf= [50],bagging_fraction= [0.5],learning_rate= [0.001] ))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# SVM classification pipeline \n",
        "full_pipline_svm = Pipeline(\n",
        "    steps=[\n",
        "        #('sampler', RandomUnderSampler(random_state=42)),\n",
        "        ('my_classifier', svm.SVC( kernel='poly', gamma=0.1) )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# MLP classification pipeline \n",
        "full_pipline_mlp = Pipeline(\n",
        "    steps=[\n",
        "        #('sampler', RandomUnderSampler(random_state=42)),\n",
        "        ('my_classifier', MLPClassifier(max_iter=100) )\n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfADqhtLRaOw"
      },
      "source": [
        "A. Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-WN0ktzRZND"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pickle\n",
        "# XGBOOST hyperparameter search space\n",
        "param_grid_xgb = {\n",
        "    'my_classifier__n_estimators': list(range(50,145,5)),  # num of estimators in xgboost is not usully set more than 100 ## `__` denotes attribute \n",
        "    'my_classifier__max_depth':list(range(5,10)),  # maximum depth of each tree is set to 20\n",
        "    'sampler__sampling_strategy':[0.9]\n",
        "}\n",
        "\n",
        "# Random forest hyperparameter search space\n",
        "param_grid_rf = {\n",
        "                 'my_classifier__n_estimators': list(range(50,120)),  # num of estimators in xgboost is not usully set more than 100 ## `__` denotes attribute \n",
        "                 'my_classifier__max_features' : ['auto'], # In random forest, tricks to determine the maximum number of features\n",
        "                 'my_classifier__max_depth': [None], # In random forest, maximum depth can be either set automatically or usually very small (eg 3 or 5)\n",
        "                 'my_classifier__min_impurity_decrease': [0.0], # in random forests, minimum impurity to keep on dissecting branches are usually set below 0.5\n",
        "                #  'sampler__sampling_strategy':(0.01*np.asarray(list(range(75,85)))).tolist(),#[0.75,0.8,0.85,0.9,0.95],\n",
        "                 'my_classifier__bootstrap':[True,False]\n",
        "}\n",
        "\n",
        "# lightGBM hyperparameter search space\n",
        "param_grid_lgbm ={\n",
        "                  'my_classifier__num_leaves': list(range(20,30)), # in gradient boost, the number of leaves should be low, but in case of high num of featres, I prefer to check out the higher ones also\n",
        "                  'my_classifier__min_child_samples': list(range(100,500,100)),  # # minimum samples per each model\n",
        "                  'my_classifier__min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4], # importance weight of each model \n",
        "                  'my_classifier__subsample': list(np.arange(0.2,1,0.1)) , # subsampling probability of instances\n",
        "                  'my_classifier__colsample_bytree': list(np.arange(0.4,1,0.1)), # sampling probability of column\n",
        "                  'my_classifier__reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], # special parameters suitable to tune\n",
        "                  'my_classifier__reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100], # special parameters suitable to tune\n",
        "                  # 'sampler__sampling_strategy':(0.01*np.asarray(list(range(75,85)))).tolist() \n",
        "                  \n",
        "}\n",
        "\n",
        "# SVM hyperparameter search space\n",
        "param_grid_svm = {\n",
        "                  'my_classifier__kernel':['poly'],\n",
        "                  'my_classifier__C': list(range(9,12)), # is the parameter for the soft margin cost function, which controls the influence of each individual support vector. It is usually selected low not to provoke underfitting\n",
        "                  'my_classifier__gamma':  np.arange(1e-5,1e-4,1e-5) , # weight of each input in RBF kernel mode. I chose a wide range of values for tuning\n",
        "                  'my_classifier__class_weight':[ None], #  to balance number of instances per each class\n",
        "                 # 'sampler__sampling_strategy':(0.01*np.asarray(list(range(83,93)))).tolist() \n",
        "}\n",
        "\n",
        "# MLP hyperparameter search space\n",
        "param_grid_mlp = {\n",
        "    'my_classifier__hidden_layer_sizes': [(10,30,10) ],\n",
        "    'my_classifier__alpha': 0.001*np.asarray(range(5,1000,5)),\n",
        "    'my_classifier__learning_rate': ['constant','adaptive'],\n",
        "    #'sampler__sampling_strategy':[ 0.8]\n",
        "\n",
        "}\n",
        "\n",
        "## Initialize GridSearchCV class to tune hyperparameters \n",
        "classifiers_names=['xgb','rf','lgbm','svm','mlp'] \n",
        "case_study=1  \n",
        "best_scores1,bestparams1=[],[]\n",
        "for iii,(full_pipeline,param_grid) in enumerate(list(zip([full_pipline_xgb,full_pipline_rf,full_pipline_lgbm,full_pipline_svm,full_pipline_mlp],[param_grid_xgb,param_grid_rf,param_grid_lgbm,param_grid_svm,param_grid_mlp]))):\n",
        "  if iii in [0]:\n",
        "    continue\n",
        "  grid_search = GridSearchCV(\n",
        "      full_pipeline, param_grid, cv=5, verbose=3, n_jobs=2, \n",
        "      )\n",
        "\n",
        "  # run the grid search object\n",
        "  grid_search.fit(train_X,train_Y)\n",
        "  val_scores = cross_val_score(grid_search, test_X, test_Y, cv=5)\n",
        "\n",
        "  best_scores1.append(grid_search.best_score_)\n",
        "  best_scores1.append(val_scores.mean())\n",
        "  bestparams1.append(grid_search.best_params_)\n",
        "  print(\"val_score: %0.2f accuracy with a standard deviation of %0.2f\" % (val_scores.mean(), val_scores.std()))\n",
        "  print('%d___%f___'%(case_study,float(grid_search.best_score_)) + classifiers_names[iii]+ ': best score {}'.format(grid_search.best_score_))\n",
        "  print('%d___%f___'%(case_study,float(grid_search.best_score_)) + classifiers_names[iii]+': best params {}'.format(grid_search.best_params_))\n",
        "\n",
        "  pickle.dump(grid_search,open('%d.pkl'%(iii),'wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuHy5c4UReP4"
      },
      "source": [
        "Grid Result:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "val_score: 0.18 accuracy with a standard deviation of 0.00\n",
        "\n",
        "1___0.134975___xgb: best score 0.134975\n",
        "\n",
        "1___0.134975___xgb: best params {'my_classifier__max_depth': 5, 'my_classifier__n_estimators': 10}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "val_score: -0.91 accuracy with a standard deviation of 0.45\n",
        "\n",
        "1___-0.233610___rf: best score -0.23360989928754777\n",
        "\n",
        "1___-0.233610___rf: best params {'my_classifier__bootstrap': True, 'my_classifier__max_depth': None, 'my_classifier__max_features': 'auto', 'my_classifier__min_impurity_decrease': 0.0, 'my_classifier__n_estimators': 140}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.18 accuracy with a standard deviation of 0.00\n",
        "\n",
        "1___0.180613___lgbm: best score 0.1806125\n",
        "\n",
        "1___0.180613___lgbm: best params {'my_classifier__colsample_bytree': 0.5, 'my_classifier__min_child_samples': 300, 'my_classifier__min_child_weight': 0.001, 'my_classifier__num_leaves': 10, 'my_classifier__reg_alpha': 10, 'my_classifier__reg_lambda': 0, 'my_classifier__subsample': 0.5}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.10 accuracy with a standard deviation of 0.01\n",
        "\n",
        "1___0.165325___svm: best score 0.165325\n",
        "\n",
        "1___0.165325___svm: best params {'my_classifier__C': 9, 'my_classifier__class_weight': None, 'my_classifier__gamma': 1e-05, 'my_classifier__kernel': 'poly'}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.05 accuracy with a standard deviation of 0.04\n",
        "\n",
        "1___0.069013___mlp: best score 0.0690125\n",
        "\n",
        "1___0.069013___mlp: best params {'my_classifier__alpha': 0.92, 'my_classifier__hidden_layer_sizes': (10, 30, 10), 'my_classifier__learning_rate': 'adaptive'}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "P.S. This is the result with \"sampler\". However, after I comments \"sampler\" in each model, results are not shown up until the program killed itself after 8hrs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XYSDgAQRlvK"
      },
      "source": [
        "B. Random Search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2XOYpUgSU8O"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pickle\n",
        "# XGBOOST hyperparameter search space\n",
        "param_rndsrch_xgb = {\n",
        "    'my_classifier__n_estimators': [100],  # num of estimators in xgboost is not usully set more than 100 ## `__` denotes attribute \n",
        "    'my_classifier__max_depth':[50] # maximum depth of each tree is set to 20\n",
        "}\n",
        "\n",
        "# Random forest hyperparameter search space\n",
        "param_rndsrch_rf = {\n",
        "                 'my_classifier__max_features' : ['auto'], #['auto''sqrt', 'log2'], # In random forest, tricks to determine the maximum number of features\n",
        "                 'my_classifier__max_depth': [None], #[None] In random forest, maximum depth can be either set automatically or usually very small (eg 3 or 5)\n",
        "                 'my_classifier__min_impurity_decrease': [1e-07] # in random forests, minimum impurity to keep on dissecting branches are usually set below 0.5\n",
        "}\n",
        "\n",
        "# lightGBM hyperparameter search space ##TODO: NOT CHECKED  ERROR NAASHENA MIDE\n",
        "param_rndsrch_lgbm ={\n",
        "                  'my_classifier__num_leaves': [10], # in gradient boost, the number of leaves should be low, but in case of high num of featres, I prefer to check out the higher ones also\n",
        "                  'my_classifier__min_child_samples': [10],  # # minimum samples per each model\n",
        "                  'my_classifier__min_child_weight': [0.00], # importance weight of each model \n",
        "                  'my_classifier__subsample': [0.6], # subsampling probability of instances\n",
        "                  'my_classifier__colsample_bytree': [0.5], # sampling probability of column\n",
        "                  'my_classifier__reg_alpha': [50], # special parameters suitable to tune\n",
        "                  'my_classifier__reg_lambda': [10], # special parameters suitable to tune\n",
        "}\n",
        "\n",
        "\n",
        "# SVM hyperparameter search space\n",
        "param_rndsrch_svm = {\n",
        "                  'my_classifier__C': [0.01,0.00001], # is the parameter for the soft margin cost function, which controls the influence of each individual support vector. It is usually selected low not to provoke underfitting\n",
        "                  'my_classifier__gamma': [1,0.01, 0.001], # weight of each input in RBF kernel mode. I chose a wide range of values for tuning\n",
        "                  'my_classifier__class_weight':['balanced',None], #  to balance number of instances per each class\n",
        "}\n",
        "\n",
        "# MLP hyperparameter search space\n",
        "param_rndsrch_mlp = {\n",
        "    'my_classifier__hidden_layer_sizes': [(34,32,32,32,17)],#[(100,),(1000,),(50,),(70,),(200,),(50,60)],\n",
        "    'my_classifier__alpha': [0.001],#[0.0001,0.001,0.01,0.15, 0.1,0.06],\n",
        "    'my_classifier__learning_rate': ['constant','adaptive']\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "## Initialize RandomizedSearchCV class to tune hyperparameters \n",
        "classifiers_names=['xgb','rf','lgbm','svm','mlp'] \n",
        "case_study=2 \n",
        "best_scores2,bestparams2=[],[]\n",
        "#for iii,(full_pipeline,param_rndsrch) in enumerate(list(zip([full_pipline_xgb,full_pipline_rf,full_pipline_lgbm,full_pipline_svm,full_pipline_mlp],[param_rndsrch_xgb,param_rndsrch_rf,param_rndsrch_lgbm,param_rndsrch_svm,param_rndsrch_mlp]))):\n",
        "for iii,(full_pipeline,param_rndsrch) in enumerate(list(zip([full_pipline_xgb],[param_rndsrch_xgb]))):\n",
        "\n",
        "  if iii<=-1:\n",
        "    best_scores2.append(None)\n",
        "    bestparams2.append(None)\n",
        "    continue\n",
        "\n",
        "  random_search = RandomizedSearchCV(full_pipeline, param_distributions=param_rndsrch, n_iter=500, cv=5, verbose=3, n_jobs=2)\n",
        "\n",
        "  # run the garch.predict(data_test)[:,1]rid search object\n",
        "  random_search.fit(train_X,train_Y)\n",
        "  val_scores = cross_val_score(random_search, test_X, test_Y, cv=5)\n",
        "\n",
        "  best_scores2.append(random_search.best_score_)\n",
        "  best_scores2.append(val_scores.mean())\n",
        "  bestparams2.append(random_search.best_params_)\n",
        "  print('-----------------------------------------------')\n",
        "  print(\"val_score: %0.2f accuracy with a standard deviation of %0.2f\" % (val_scores.mean(), val_scores.std()))\n",
        "  print('%d___%f___'%(case_study,float(random_search.best_score_)) + classifiers_names[iii]+ ': best score {}'.format(random_search.best_score_))\n",
        "  print('%d___%f___'%(case_study,float(random_search.best_score_)) + classifiers_names[iii]+': best params {}'.format(random_search.best_params_))\n",
        "  print('-----------------------------------------------')\n",
        "\n",
        "  #pickle.dump(random_search,open('%d___%d.pkl'%(iii,case_study),'wb')) # profiling the results in a formatted way"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnfnxKg9SqTT"
      },
      "source": [
        "Random Search Result:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**XGB**:\n",
        "val_score: 0.50 accuracy with a standard deviation of 0.01\n",
        "\n",
        "2___0.496200___xgb: best score 0.4962\n",
        "\n",
        "2___0.496200___xgb: best params {'my_classifier__n_estimators': 1, 'my_classifier__max_depth': 5}\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "val_score: 0.56 accuracy with a standard deviation of 0.01\n",
        "\n",
        "2___0.581950___xgb: best score 0.5819500000000001\n",
        "\n",
        "2___0.581950___xgb: best params {'my_classifier__n_estimators': 3, 'my_classifier__max_depth': 13}\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "val_score: 0.60 accuracy with a standard deviation of 0.01\n",
        "\n",
        "2___0.648925___xgb: best score 0.648925\n",
        "\n",
        "2___0.648925___xgb: best params {'my_classifier__n_estimators': 10, 'my_classifier__max_depth': 50}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**val_score: 0.62 accuracy with a standard deviation of 0.01**\n",
        "\n",
        "**2___0.667450___xgb: best score 0.66745**\n",
        "\n",
        "**2___0.667450___xgb: best params {'my_classifier__n_estimators': 100, 'my_classifier__max_depth': 20})**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**RF**\n",
        "val_score: 0.72 accuracy with a standard deviation of 0.00\n",
        "\n",
        "2___0.723599___rf: best score 0.723599155416591\n",
        "\n",
        "2___0.723599___rf: best params {'my_classifier__min_impurity_decrease': 0.001, 'my_classifier__max_features': 'auto', 'my_classifier__max_depth': 4}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "val_score: 0.82 accuracy with a standard deviation of 0.00\n",
        "\n",
        "2___0.800333___rf: best score 0.8003326578235643\n",
        "\n",
        "2___0.800333___rf: best params {'my_classifier__min_impurity_decrease': 0.005, 'my_classifier__max_features': 'auto', 'my_classifier__max_depth': None}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.90 accuracy with a standard deviation of 0.00\n",
        "\n",
        "2___0.920077___rf: best score 0.9200768975273762\n",
        "\n",
        "2___0.920077___rf: best params {'my_classifier__min_impurity_decrease': 1e-06, 'my_classifier__max_features': 'auto', 'my_classifier__max_depth': None}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**val_score: 0.90 accuracy with a standard deviation of 0.00**\n",
        "\n",
        "**2___0.920111___rf: best score 0.9201108383565064**\n",
        "\n",
        "**2___0.920111___rf: best params {'my_classifier__min_impurity_decrease': 1e-07, 'my_classifier__max_features': 'auto', 'my_classifier__max_depth': None}**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**LSTM**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**SVM**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**MLP**：\n",
        "val_score: 0.60 accuracy with a standard deviation of 0.00\n",
        "\n",
        "2___0.618437___mlp: best score 0.6184375\n",
        "\n",
        "2___0.618437___mlp: best params {'my_classifier__learning_rate': 'constant', 'my_classifier__hidden_layer_sizes': (10, 40, 30), 'my_classifier__alpha': 0.001}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.60 accuracy with a standard deviation of 0.00\n",
        "\n",
        "2___0.628713___mlp: best score 0.6287125\n",
        "\n",
        "2___0.628713___mlp: best params {'my_classifier__learning_rate': 'constant', 'my_classifier__hidden_layer_sizes': (200,), 'my_classifier__alpha': 0.001}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.60 accuracy with a standard deviation of 0.00\n",
        "\n",
        "2___0.626100___mlp: best score 0.6261\n",
        "\n",
        "2___0.626100___mlp: best params {'my_classifier__learning_rate': 'adaptive', 'my_classifier__hidden_layer_sizes': (50,), 'my_classifier__alpha': 0.001}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.60 accuracy with a standard deviation of 0.01\n",
        "\n",
        "2___0.628175___mlp: best score 0.628175\n",
        "\n",
        "2___0.628175___mlp: best params {'my_classifier__learning_rate': 'adaptive', 'my_classifier__hidden_layer_sizes': (100,), 'my_classifier__alpha': 0.01}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.61 accuracy with a standard deviation of 0.00\n",
        "\n",
        "2___0.627062___mlp: best score 0.6270625\n",
        "\n",
        "2___0.627062___mlp: best params {'my_classifier__learning_rate': 'constant', 'my_classifier__hidden_layer_sizes': (100,), 'my_classifier__alpha': 0.0001}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "2___0.628063___mlp: best score 0.6280625000000001\n",
        "\n",
        "2___0.628063___mlp: best params {'my_classifier__learning_rate': 'adaptive', 'my_classifier__hidden_layer_sizes': (100,), 'my_classifier__alpha': 0.001}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**val_score: 0.62 accuracy with a standard deviation of 0.01**\n",
        "\n",
        "**2___0.686600___mlp: best score 0.6866**\n",
        "\n",
        "**2___0.686600___mlp: best params {'my_classifier__learning_rate': 'constant', 'my_classifier__hidden_layer_sizes': (34, 68, 17), 'my_classifier__alpha': 0.001}**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-5GO5Q3SvoE"
      },
      "source": [
        "C. Bayesian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_18BXHEpSw_l",
        "outputId": "f5075591-e26b-483a-ab7a-0c22291a8997"
      },
      "source": [
        "!pip install scikit-optimize\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pickle\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# XGBOOST hyperparameter search space\n",
        "param_bayesS_xgb = {\n",
        "    'my_classifier__n_estimators': [70],  # num of estimators in xgboost is not usully set more than 100 ## `__` denotes attribute \n",
        "    'my_classifier__max_depth': [15], # maximum depth of each tree is set to 20\n",
        "\n",
        "    #'my_classifier__eta': [0.1,0.001,0.0001,0.5],\n",
        "    #'my_classifier__gamma': [0,10,20,100],\n",
        "    #'my_classifier__min_child_weight':[1,10,20],\n",
        "    #'my_classifier__subsample': [0.7,0.8,0.9],\n",
        "    #'my_classifier__colsample_bytree': [0.8,1,0.2,0.1],\n",
        "}\n",
        "\n",
        "# Random forest hyperparameter search space\n",
        "param_bayesS_rf = {\n",
        "                 'my_classifier__max_features': ['auto', 'sqrt', 'log2'], # In random forest, tricks to determine the maximum number of features\n",
        "                 'my_classifier__max_depth': [None,5], # In random forest, maximum depth can be either set automatically or usually very small (eg 3 or 5)\n",
        "                 'my_classifier__min_impurity_decrease': [10.0], # in random forests, minimum impurity to keep on dissecting branches are usually set below 0.5\n",
        "                 #'my_classifier__min_samples_split': [10],\n",
        "}\n",
        "\n",
        "# lightGBM hyperparameter search space\n",
        "param_bayesS_lgbm ={\n",
        "                  'my_classifier__num_leaves': [100], # in gradient boost, the number of leaves should be low, but in case of high num of featres, I prefer to check out the higher ones also\n",
        "                  'my_classifier__min_child_samples': [400],  # # minimum samples per each model\n",
        "                  'my_classifier__min_child_weight': [1], # importance weight of each model \n",
        "                  'my_classifier__subsample': [0.8], # subsampling probability of instances\n",
        "                  'my_classifier__colsample_bytree': [0.6], # sampling probability of column\n",
        "                  'my_classifier__reg_alpha': [0.01], # special parameters suitable to tune\n",
        "                  'my_classifier__reg_lambda': [0.001], # special parameters suitable to tune\n",
        "}\n",
        "\n",
        "\n",
        "# SVM hyperparameter search space\n",
        "param_bayesS_svm = {\n",
        "                  'my_classifier__C': [20.0], # is the parameter for the soft margin cost function, which controls the influence of each individual support vector. It is usually selected low not to provoke underfitting\n",
        "                  'my_classifier__gamma': ['scale','auto'], #'auto'# weight of each input in RBF kernel mode. I chose a wide range of values for tuning\n",
        "                  'my_classifier__class_weight':[None], #  to balance number of instances per each class\n",
        "                  'my_classifier__degree': [10],\n",
        "                  'my_classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "}\n",
        "\n",
        "# MLP hyperparameter search space\n",
        "param_bayesS_mlp = {\n",
        "    'my_classifier__hidden_layer_sizes': [10,20],\n",
        "    'my_classifier__alpha': [0.1],\n",
        "    'my_classifier__learning_rate': ['constant','adaptive','invscaling'],\n",
        "    'my_classifier__solver': ['sgd','adam','lbfgs']\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "## Initialize RandomizedSearchCV class to tune hyperparameters \n",
        "#classifiers_names=['xgb','rf','lgbm','svm','mlp']\n",
        "classifiers_names=['xgb','rf','mlp']\n",
        "\n",
        "case_study=3 \n",
        "best_scores3,bestparams3=[],[]\n",
        "#for iii,(full_pipeline,param_bayesS) in enumerate(list(zip([full_pipline_xgb,full_pipline_rf,full_pipline_lgbm,full_pipline_svm,full_pipline_mlp],[param_bayesS_xgb,param_bayesS_rf,param_bayesS_lgbm,param_bayesS_svm,param_bayesS_mlp]))):\n",
        "for iii,(full_pipeline,param_bayesS) in enumerate(list(zip([full_pipline_xgb,full_pipline_rf,full_pipline_mlp],[param_bayesS_xgb,param_bayesS_rf,param_bayesS_mlp]))):\n",
        "  if iii<=-1:\n",
        "    best_scores3.append(None)\n",
        "    bestparams3.append(None)\n",
        "    continue\n",
        "\n",
        "  bayes_search = BayesSearchCV(full_pipeline, param_bayesS, n_iter=20, cv=5, verbose=3, n_jobs=2)\n",
        "\n",
        "  # run the grid search object\n",
        "  bayes_search.fit(train_X,train_Y)\n",
        "  val_scores = cross_val_score(bayes_search, test_X, test_Y, cv=5)\n",
        "  best_scores3.append(bayes_search.best_score_)\n",
        "  best_scores3.append(val_scores.mean())\n",
        "  bestparams3.append(bayes_search.best_params_)\n",
        "  print('-----------------------------------------------')\n",
        "  print(\"val_score: %0.2f accuracy with a standard deviation of %0.2f\" % (val_scores.mean(), val_scores.std()))\n",
        "  print('%d___%f___'%(case_study,float(bayes_search.best_score_)) + classifiers_names[iii]+ ': best score {}'.format(bayes_search.best_score_))\n",
        "  print('%d___%f___'%(case_study,float(bayes_search.best_score_)) + classifiers_names[iii]+': best params {}'.format(bayes_search.best_params_))\n",
        "  print('-----------------------------------------------')\n",
        "\n",
        "  pickle.dump(bayes_search,open('%d___%d.pkl'%(iii,case_study),'wb'))\n",
        "# define ranges for bayes search\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.8.1-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 81 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 101 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.19.5)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.8.3-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-21.8.3 scikit-optimize-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo2PsuaPS-UY"
      },
      "source": [
        "Bayesian Result:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**RF**\n",
        "val_score: 0.43 accuracy with a standard deviation of 0.02\n",
        "\n",
        "3___0.403566___rf: best score 0.4035659718105104\n",
        "\n",
        "3___0.403566___rf: best params OrderedDict([('my_classifier__max_features', 'auto'), ('my_classifier__min_impurity_decrease', 0.8)])\n",
        "\n",
        "time:  0:11:35.557117\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "val_score: 0.50 accuracy with a standard deviation of 0.01\n",
        "\n",
        "3___0.501343___rf: best score 0.5013426384956406\n",
        "\n",
        "3___0.501343___rf: best params OrderedDict([('my_classifier__max_features', 'auto'), ('my_classifier__min_impurity_decrease', 0.5)])\n",
        "\n",
        "time:  0:13:28.250860\n",
        "\n",
        "---\n",
        "val_score: 0.57 accuracy with a standard deviation of 0.01\n",
        "\n",
        "3___0.579447___rf: best score 0.5794466100427025\n",
        "\n",
        "3___0.579447___rf: best params OrderedDict([('my_classifier__max_features', 'auto'), ('my_classifier__min_impurity_decrease', 0.3775439910834993)])\n",
        "\n",
        "time:  0:14:23.430465\n",
        "\n",
        "---\n",
        "val_score: 0.90 accuracy with a standard deviation of 0.00\n",
        "\n",
        "\n",
        "3___0.920170___rf: best score 0.9201696492597291\n",
        "\n",
        "3___0.920170___rf: best params OrderedDict([('my_classifier__max_features', 'auto'), ('my_classifier__min_impurity_decrease', 0.0)])\n",
        "\n",
        "time: time:  0:42:32.451886\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "val_score: 0.90 accuracy with a standard deviation of 0.00\n",
        "\n",
        "3___0.920229___rf: best score 0.9202292828316108\n",
        "\n",
        "3___0.920229___rf: best params OrderedDict([('my_classifier__max_features', 'auto'), ('my_classifier__min_impurity_decrease', 0.0)])\n",
        "\n",
        "time:  0:35:58.411620\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "val_score: 0.90 accuracy with a standard deviation of 0.00\n",
        "\n",
        "3___0.920318___rf: best score 0.9203176191980185\n",
        "\n",
        "3___0.920318___rf: best params OrderedDict([('my_classifier__max_features', 'auto'), ('my_classifier__min_impurity_decrease', 0.0), ('my_classifier__min_samples_split', 10)])\n",
        "\n",
        "time:  1:20:58.912511"
      ]
    }
  ]
}
